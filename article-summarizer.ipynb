{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Mining and Applied NLP (44-620)\n",
    "\n",
    "## Final Project: Article Summarizer\n",
    "\n",
    "### Student Name: Jake Rood\n",
    "\n",
    "### GitHub Repository: [jakerood/article-summarizer-final-project](https://github.com/jakerood/article-summarizer-final-project)\n",
    "\n",
    "Perform the tasks described in the Markdown cells below.  When you have completed the assignment make sure your code cells have all been run (and have output beneath them) and ensure you have committed and pushed ALL of your changes to your assignment repository.\n",
    "\n",
    "You should bring in code from previous assignments to help you answer the questions below.\n",
    "\n",
    "Every question that requires you to write code will have a code cell underneath it; you may either write your entire solution in that cell or write it in a python file (`.py`), then import and run the appropriate code to answer the question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test for Import Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All prereqs installed.\n",
      "Package                      Version\n",
      "---------------------------- -----------\n",
      "absl-py                      1.4.0\n",
      "annotated-types              0.5.0\n",
      "anyio                        3.7.1\n",
      "appdirs                      1.4.4\n",
      "appnope                      0.1.3\n",
      "argon2-cffi                  23.1.0\n",
      "argon2-cffi-bindings         21.2.0\n",
      "arrow                        1.2.3\n",
      "astroid                      2.15.6\n",
      "asttokens                    2.2.1\n",
      "astunparse                   1.6.3\n",
      "async-lru                    2.0.4\n",
      "attrs                        23.1.0\n",
      "Babel                        2.12.1\n",
      "backcall                     0.2.0\n",
      "beautifulsoup4               4.12.2\n",
      "black                        24.3.0\n",
      "bleach                       6.0.0\n",
      "blis                         0.7.10\n",
      "cachetools                   5.3.1\n",
      "catalogue                    2.0.9\n",
      "certifi                      2023.7.22\n",
      "cffi                         1.15.1\n",
      "charset-normalizer           3.2.0\n",
      "click                        8.1.7\n",
      "cloudpathlib                 0.15.1\n",
      "comm                         0.1.4\n",
      "confection                   0.1.1\n",
      "contourpy                    1.1.0\n",
      "cryptography                 41.0.4\n",
      "cycler                       0.11.0\n",
      "cymem                        2.0.7\n",
      "debugpy                      1.6.7.post1\n",
      "decorator                    5.1.1\n",
      "defusedxml                   0.7.1\n",
      "Deprecated                   1.2.14\n",
      "dill                         0.3.7\n",
      "dnspython                    2.4.2\n",
      "dodgy                        0.2.1\n",
      "dweepy                       0.3.0\n",
      "en-core-web-sm               3.7.1\n",
      "executing                    1.2.0\n",
      "fastjsonschema               2.18.0\n",
      "flake8                       5.0.4\n",
      "flake8-polyfill              1.0.2\n",
      "flatbuffers                  23.5.26\n",
      "fonttools                    4.42.1\n",
      "fqdn                         1.5.1\n",
      "gast                         0.4.0\n",
      "geographiclib                2.0\n",
      "geopy                        2.4.0\n",
      "gitdb                        4.0.10\n",
      "GitPython                    3.1.32\n",
      "google-auth                  2.22.0\n",
      "google-auth-oauthlib         1.0.0\n",
      "google-pasta                 0.2.0\n",
      "grpcio                       1.57.0\n",
      "h11                          0.14.0\n",
      "h5py                         3.9.0\n",
      "httpcore                     1.0.4\n",
      "httpx                        0.27.0\n",
      "idna                         3.4\n",
      "imageio                      2.31.5\n",
      "ipykernel                    6.29.4\n",
      "ipython                      8.14.0\n",
      "ipywidgets                   8.1.2\n",
      "isoduration                  20.11.0\n",
      "isort                        5.12.0\n",
      "jedi                         0.19.0\n",
      "Jinja2                       3.1.2\n",
      "joblib                       1.3.2\n",
      "json5                        0.9.14\n",
      "jsonpointer                  2.4\n",
      "jsonschema                   4.19.0\n",
      "jsonschema-specifications    2023.7.1\n",
      "jupyter                      1.0.0\n",
      "jupyter_client               8.3.0\n",
      "jupyter-console              6.6.3\n",
      "jupyter_core                 5.3.1\n",
      "jupyter-events               0.7.0\n",
      "jupyter-lsp                  2.2.0\n",
      "jupyter_server               2.7.2\n",
      "jupyter_server_terminals     0.4.4\n",
      "jupyterlab                   4.1.6\n",
      "jupyterlab-pygments          0.2.2\n",
      "jupyterlab_server            2.24.0\n",
      "jupyterlab_widgets           3.0.10\n",
      "keras                        2.13.1\n",
      "kiwisolver                   1.4.5\n",
      "langcodes                    3.3.0\n",
      "lazy-object-proxy            1.9.0\n",
      "libclang                     16.0.6\n",
      "lxml                         4.9.3\n",
      "Markdown                     3.4.4\n",
      "MarkupSafe                   2.1.3\n",
      "matplotlib                   3.8.3\n",
      "matplotlib-inline            0.1.6\n",
      "mccabe                       0.7.0\n",
      "mistune                      3.0.1\n",
      "murmurhash                   1.0.9\n",
      "mypy-extensions              1.0.0\n",
      "nbclient                     0.7.4\n",
      "nbconvert                    7.7.4\n",
      "nbformat                     5.9.2\n",
      "nest-asyncio                 1.5.7\n",
      "nltk                         3.8.1\n",
      "notebook                     7.1.2\n",
      "notebook_shim                0.2.3\n",
      "numpy                        1.26.4\n",
      "oauthlib                     3.2.2\n",
      "opt-einsum                   3.3.0\n",
      "overrides                    7.4.0\n",
      "packaging                    23.1\n",
      "pandas                       2.2.1\n",
      "pandocfilters                1.5.0\n",
      "parso                        0.8.3\n",
      "pathspec                     0.11.2\n",
      "pathy                        0.10.2\n",
      "pep8-naming                  0.10.0\n",
      "pexpect                      4.8.0\n",
      "pickleshare                  0.7.5\n",
      "Pillow                       10.0.0\n",
      "pip                          24.0\n",
      "platformdirs                 3.10.0\n",
      "preshed                      3.0.8\n",
      "prometheus-client            0.17.1\n",
      "prompt-toolkit               3.0.39\n",
      "prospector                   1.10.2\n",
      "protobuf                     4.24.1\n",
      "psutil                       5.9.5\n",
      "ptyprocess                   0.7.0\n",
      "pure-eval                    0.2.2\n",
      "py4j                         0.10.9.7\n",
      "pyarrow                      13.0.0\n",
      "pyasn1                       0.5.0\n",
      "pyasn1-modules               0.3.0\n",
      "pybaseball                   2.2.7\n",
      "pycodestyle                  2.9.1\n",
      "pycparser                    2.21\n",
      "pydantic                     2.3.0\n",
      "pydantic_core                2.6.3\n",
      "pydocstyle                   6.3.0\n",
      "pyflakes                     2.5.0\n",
      "PyGithub                     1.59.1\n",
      "Pygments                     2.16.1\n",
      "PyHyphen                     4.0.3\n",
      "PyJWT                        2.8.0\n",
      "pylint                       2.17.5\n",
      "pylint-celery                0.3\n",
      "pylint-django                2.5.3\n",
      "pylint-flask                 0.6\n",
      "pylint-plugin-utils          0.7\n",
      "pymongo                      4.5.0\n",
      "PyNaCl                       1.5.0\n",
      "pyparsing                    3.0.9\n",
      "pyspark                      3.4.1\n",
      "python-dateutil              2.8.2\n",
      "python-json-logger           2.0.7\n",
      "pytz                         2023.3\n",
      "PyYAML                       6.0.1\n",
      "pyzmq                        25.1.1\n",
      "qtconsole                    5.5.1\n",
      "QtPy                         2.4.1\n",
      "referencing                  0.30.2\n",
      "regex                        2023.8.8\n",
      "requests                     2.31.0\n",
      "requests-oauthlib            1.3.1\n",
      "requirements-detector        1.2.2\n",
      "rfc3339-validator            0.1.4\n",
      "rfc3986-validator            0.1.1\n",
      "rpds-py                      0.9.2\n",
      "rsa                          4.9\n",
      "ruff                         0.3.5\n",
      "scikit-learn                 1.3.1\n",
      "scipy                        1.11.3\n",
      "seaborn                      0.13.2\n",
      "semver                       3.0.1\n",
      "Send2Trash                   1.8.2\n",
      "setoptconf-tmp               0.3.1\n",
      "setuptools                   65.5.0\n",
      "six                          1.16.0\n",
      "smart-open                   6.3.0\n",
      "smmap                        5.0.0\n",
      "sniffio                      1.3.0\n",
      "snowballstemmer              2.2.0\n",
      "soupsieve                    2.4.1\n",
      "spacy                        3.7.4\n",
      "spacy-legacy                 3.0.12\n",
      "spacy-loggers                1.0.4\n",
      "spacytextblob                4.0.0\n",
      "srsly                        2.4.7\n",
      "stack-data                   0.6.2\n",
      "tensorboard                  2.13.0\n",
      "tensorboard-data-server      0.7.1\n",
      "tensorflow                   2.13.0\n",
      "tensorflow-estimator         2.13.0\n",
      "tensorflow-io-gcs-filesystem 0.33.0\n",
      "termcolor                    2.3.0\n",
      "terminado                    0.17.1\n",
      "textatistic                  0.0.1\n",
      "textblob                     0.15.3\n",
      "thinc                        8.2.3\n",
      "threadpoolctl                3.2.0\n",
      "tinycss2                     1.2.1\n",
      "toml                         0.10.2\n",
      "tomlkit                      0.12.1\n",
      "tornado                      6.3.3\n",
      "tqdm                         4.66.1\n",
      "traitlets                    5.9.0\n",
      "typer                        0.7.0\n",
      "typing_extensions            4.8.0\n",
      "tzdata                       2023.3\n",
      "uri-template                 1.3.0\n",
      "urllib3                      1.26.16\n",
      "voila                        0.5.5\n",
      "wasabi                       1.1.2\n",
      "wcwidth                      0.2.6\n",
      "weasel                       0.3.1\n",
      "webcolors                    1.13\n",
      "webencodings                 0.5.1\n",
      "websocket-client             1.6.2\n",
      "websockets                   11.0.3\n",
      "Werkzeug                     2.3.7\n",
      "wheel                        0.41.2\n",
      "widgetsnbextension           4.0.10\n",
      "wrapt                        1.15.0\n"
     ]
    }
   ],
   "source": [
    "# Create and activate a Python virtual environment. \n",
    "# Before starting the final project, try these imports FIRST\n",
    "# Address any errors you get running this code cell \n",
    "# by installing the necessary packages into your active Python virtual environment.\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import requests\n",
    "import spacy\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "\n",
    "print('All prereqs installed.')\n",
    "\n",
    "!pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #1\n",
    "Find on the internet an article or blog post about a topic that interests you and you are able to get the text for using the technologies we have applied in the course.  Get the html for the article and store it in a file (which you must submit with your project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article content dumped to article_content.pkl\n"
     ]
    }
   ],
   "source": [
    "# URL of the webpage\n",
    "url = \"https://sports.yahoo.com/caitlin-clark-effect-set-transform-214638344.html\"\n",
    "\n",
    "# Send a request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Confirm if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content\n",
    "    html_content = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Find the article content\n",
    "    article_content = html_content.find(\"article\")\n",
    "\n",
    "    # Check if the article content is found\n",
    "    if article_content:\n",
    "        # Dump the article content into a pickle file\n",
    "        with open(\"article_content.pkl\", \"wb\") as file:\n",
    "            pickle.dump(str(article_content), file)\n",
    "        print(\"Article content dumped to article_content.pkl\")\n",
    "    else:\n",
    "        print(\"Article content not found\")\n",
    "else:\n",
    "    print(\"Error:\", response.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #2\n",
    "Read in your article's html source from the file you created in question 1 and do sentiment analysis on the article/post's text (use `.get_text()`).  Print the polarity score with an appropriate label.  Additionally print the number of sentences in the original article (with an appropriate label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polarity Score: 0.12739898989898987\n",
      "Number of Sentences in the Original Article: 32\n"
     ]
    }
   ],
   "source": [
    "# Read in the article from the pickle file\n",
    "with open(\"article_content.pkl\", \"rb\") as file:\n",
    "    article_html = pickle.load(file)\n",
    "\n",
    "# Convert the HTML to a BeautifulSoup object\n",
    "soup = BeautifulSoup(article_html, \"html.parser\")\n",
    "\n",
    "# Get the text from the article content\n",
    "article_text = soup.get_text()\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER, and word vectors\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Add SpacyTextBlob to the pipeline\n",
    "nlp.add_pipe('spacytextblob')\n",
    "\n",
    "# Process the article text with spaCy\n",
    "doc = nlp(article_text)\n",
    "\n",
    "# Get the polarity score\n",
    "polarity = doc._.polarity\n",
    "\n",
    "# Print the polarity score with an appropriate label\n",
    "print(\"Polarity Score:\", polarity)\n",
    "\n",
    "# Count the number of sentences in the original article\n",
    "num_sentences = len(list(doc.sents))\n",
    "\n",
    "# Print the number of sentences with an appropriate label\n",
    "print(\"Number of Sentences in the Original Article:\", num_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #3\n",
    "Load the article text into a trained `spaCy` pipeline, and determine the 5 most frequent tokens (converted to lower case).  Print the common tokens with an appropriate label.  Additionally, print the tokens their frequencies (with appropriate labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Most Common Tokens:\n",
      "clark: 17\n",
      "women: 12\n",
      "basketball: 9\n",
      "college: 7\n",
      "sport: 7\n",
      "\n",
      "All Tokens and Their Frequencies:\n",
      "clark: 17\n",
      "women: 12\n",
      "basketball: 9\n",
      "college: 7\n",
      "sport: 7\n",
      "wnba: 7\n",
      "said: 6\n",
      "million: 6\n",
      "sports: 5\n",
      "going: 5\n",
      "year: 4\n",
      "kind: 4\n",
      "engelbert: 4\n",
      "media: 4\n",
      "magic: 4\n",
      "university: 4\n",
      "south: 4\n",
      "carolina: 4\n",
      "impact: 4\n",
      "game: 4\n",
      "effect: 3\n",
      "career: 3\n",
      "caitlin: 3\n",
      "pick: 3\n",
      "draft: 3\n",
      "league: 3\n",
      "rights: 3\n",
      "time: 3\n",
      "bird: 3\n",
      "season: 3\n",
      "iowa: 3\n",
      "collegiate: 3\n",
      "indiana: 3\n",
      "people: 3\n",
      "set: 2\n",
      "records: 2\n",
      "old: 2\n",
      "professional: 2\n",
      "years: 2\n",
      "told: 2\n",
      "double: 2\n",
      "fees: 2\n",
      "long: 2\n",
      "opportunity: 2\n",
      "reese: 2\n",
      "history: 2\n",
      "men: 2\n",
      "like: 2\n",
      "bring: 2\n",
      "followers: 2\n",
      "x: 2\n",
      "recent: 2\n",
      "games: 2\n",
      "according: 2\n",
      "tsunami: 2\n",
      "watched: 2\n",
      "seats: 2\n",
      "los: 2\n",
      "angeles: 2\n",
      "appeal: 2\n",
      "las: 2\n",
      "vegas: 2\n",
      "arena: 2\n",
      "kane: 2\n",
      "stop: 2\n",
      "better: 2\n",
      "invest: 2\n",
      "read: 1\n",
      "transform: 1\n",
      "wnbarob: 1\n",
      "woollardmon: 1\n",
      "apr: 1\n",
      "min: 1\n",
      "readlink: 1\n",
      "dazzling: 1\n",
      "smashed: 1\n",
      "court: 1\n",
      "legacy: 1\n",
      "trailblazing: 1\n",
      "icon: 1\n",
      "secure: 1\n",
      "prepares: 1\n",
      "chosen: 1\n",
      "week: 1\n",
      "experts: 1\n",
      "predicting: 1\n",
      "transformative: 1\n",
      "cathy: 1\n",
      "commissioner: 1\n",
      "monday: 1\n",
      "generation: 1\n",
      "players: 1\n",
      "economic: 1\n",
      "engines: 1\n",
      "ensure: 1\n",
      "financial: 1\n",
      "footing: 1\n",
      "cnbc: 1\n",
      "expects: 1\n",
      "existing: 1\n",
      "deals: 1\n",
      "value: 1\n",
      "negotiated: 1\n",
      "hope: 1\n",
      "undervalued: 1\n",
      "enormous: 1\n",
      "landscape: 1\n",
      "changing: 1\n",
      "arrival: 1\n",
      "stars: 1\n",
      "louisiana: 1\n",
      "state: 1\n",
      "angel: 1\n",
      "kamilla: 1\n",
      "cardoso: 1\n",
      "rivalry: 1\n",
      "johnson: 1\n",
      "larry: 1\n",
      "helped: 1\n",
      "create: 1\n",
      "modern: 1\n",
      "think: 1\n",
      "setting: 1\n",
      "deal: 1\n",
      "look: 1\n",
      "moment: 1\n",
      "class: 1\n",
      "built: 1\n",
      "audiences: 1\n",
      "significantly: 1\n",
      "substantial: 1\n",
      "social: 1\n",
      "followings: 1\n",
      "twitter: 1\n",
      "breanna: 1\n",
      "stewart: 1\n",
      "current: 1\n",
      "valuable: 1\n",
      "player: 1\n",
      "affectionately: 1\n",
      "nicknamed: 1\n",
      "bayou: 1\n",
      "barbie: 1\n",
      "following: 1\n",
      "business: 1\n",
      "seasons: 1\n",
      "documented: 1\n",
      "hawkeyes: 1\n",
      "broke: 1\n",
      "attendance: 1\n",
      "championship: 1\n",
      "drew: 1\n",
      "average: 1\n",
      "audience: 1\n",
      "viewers: 1\n",
      "network: 1\n",
      "program: 1\n",
      "generated: 1\n",
      "earlier: 1\n",
      "signs: 1\n",
      "starting: 1\n",
      "wash: 1\n",
      "fever: 1\n",
      "club: 1\n",
      "guaranteed: 1\n",
      "april: 1\n",
      "sold: 1\n",
      "connecticut: 1\n",
      "hours: 1\n",
      "putting: 1\n",
      "sale: 1\n",
      "courtside: 1\n",
      "indianapolis: 1\n",
      "offered: 1\n",
      "resale: 1\n",
      "website: 1\n",
      "expect: 1\n",
      "cash: 1\n",
      "box: 1\n",
      "office: 1\n",
      "aces: 1\n",
      "announced: 1\n",
      "plans: 1\n",
      "july: 1\n",
      "home: 1\n",
      "seat: 1\n",
      "michelob: 1\n",
      "ultra: 1\n",
      "bigger: 1\n",
      "capacity: 1\n",
      "t: 1\n",
      "mobile: 1\n",
      "mary: 1\n",
      "jo: 1\n",
      "professor: 1\n",
      "minnesota: 1\n",
      "founding: 1\n",
      "director: 1\n",
      "tucker: 1\n",
      "center: 1\n",
      "research: 1\n",
      "girls: 1\n",
      "says: 1\n",
      "unprecedented: 1\n",
      "influence: 1\n",
      "able: 1\n",
      "capture: 1\n",
      "lightning: 1\n",
      "bottle: 1\n",
      "national: 1\n",
      "public: 1\n",
      "radio: 1\n",
      "interview: 1\n",
      "ended: 1\n",
      "disappointment: 1\n",
      "sunday: 1\n",
      "defeat: 1\n",
      "remained: 1\n",
      "centre: 1\n",
      "attention: 1\n",
      "coach: 1\n",
      "dawn: 1\n",
      "staley: 1\n",
      "thanked: 1\n",
      "lifting: 1\n",
      "carried: 1\n",
      "heavy: 1\n",
      "load: 1\n",
      "tour: 1\n",
      "lift: 1\n",
      "derives: 1\n",
      "satisfaction: 1\n",
      "redefined: 1\n",
      "popular: 1\n",
      "given: 1\n",
      "thrives: 1\n",
      "coolest: 1\n",
      "journey: 1\n",
      "achievements: 1\n",
      "included: 1\n",
      "breaking: 1\n",
      "pete: 1\n",
      "maravich: 1\n",
      "scoring: 1\n",
      "record: 1\n",
      "started: 1\n",
      "playing: 1\n",
      "ending: 1\n",
      "tv: 1\n",
      "continues: 1\n",
      "believes: 1\n",
      "investment: 1\n",
      "key: 1\n",
      "success: 1\n",
      "term: 1\n",
      "matter: 1\n",
      "believe: 1\n",
      "things: 1\n",
      "thrive: 1\n",
      "continue: 1\n",
      "money: 1\n",
      "resources: 1\n",
      "opportunities: 1\n",
      "drive: 1\n",
      "future: 1\n",
      "sst: 1\n"
     ]
    }
   ],
   "source": [
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process the article text using spaCy\n",
    "doc = nlp(article_text)\n",
    "\n",
    "# Define a set of stopwords\n",
    "stopwords = nlp.Defaults.stop_words\n",
    "\n",
    "# Initialize a Counter to count the frequency of tokens\n",
    "token_counter = Counter()\n",
    "\n",
    "# Iterate over tokens in the document\n",
    "for token in doc:\n",
    "    # Check if the token is a word (not punctuation or whitespace) and not a stop word\n",
    "    if token.is_alpha and not token.is_stop:\n",
    "        # Update the counter with the lowercase version of the token text\n",
    "        token_counter[token.text.lower()] += 1\n",
    "\n",
    "# Get the 5 most common tokens\n",
    "most_common_tokens = token_counter.most_common(5)\n",
    "\n",
    "# Print the 5 most common tokens with an appropriate label\n",
    "print(\"5 Most Common Tokens:\")\n",
    "for token, frequency in most_common_tokens:\n",
    "    print(f\"{token}: {frequency}\")\n",
    "\n",
    "# Print all tokens and their frequencies with appropriate labels\n",
    "print(\"\\nAll Tokens and Their Frequencies:\")\n",
    "for token, frequency in token_counter.most_common():\n",
    "    print(f\"{token}: {frequency}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #4\n",
    "Load the article text into a trained `spaCy` pipeline, and determine the 5 most frequent lemmas (converted to lower case).  Print the common lemmas with an appropriate label.  Additionally, print the lemmas with their frequencies (with appropriate labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Most Common Lemmas:\n",
      "clark: 17\n",
      "sport: 12\n",
      "woman: 10\n",
      "basketball: 9\n",
      "college: 7\n",
      "\n",
      "All Lemmas and Their Frequencies:\n",
      "clark: 17\n",
      "sport: 12\n",
      "woman: 10\n",
      "basketball: 9\n",
      "college: 7\n",
      "wnba: 7\n",
      "say: 7\n",
      "year: 6\n",
      "million: 6\n",
      "game: 6\n",
      "go: 5\n",
      "kind: 4\n",
      "engelbert: 4\n",
      "magic: 4\n",
      "university: 4\n",
      "south: 4\n",
      "carolina: 4\n",
      "impact: 4\n",
      "season: 4\n",
      "effect: 3\n",
      "set: 3\n",
      "career: 3\n",
      "record: 3\n",
      "caitlin: 3\n",
      "pick: 3\n",
      "draft: 3\n",
      "league: 3\n",
      "right: 3\n",
      "opportunity: 3\n",
      "time: 3\n",
      "bird: 3\n",
      "iowa: 3\n",
      "collegiate: 3\n",
      "indiana: 3\n",
      "seat: 3\n",
      "people: 3\n",
      "old: 2\n",
      "professional: 2\n",
      "player: 2\n",
      "tell: 2\n",
      "expect: 2\n",
      "medium: 2\n",
      "deal: 2\n",
      "double: 2\n",
      "fee: 2\n",
      "women: 2\n",
      "long: 2\n",
      "media: 2\n",
      "reese: 2\n",
      "history: 2\n",
      "man: 2\n",
      "like: 2\n",
      "bring: 2\n",
      "audience: 2\n",
      "following: 2\n",
      "follower: 2\n",
      "x: 2\n",
      "recent: 2\n",
      "break: 2\n",
      "accord: 2\n",
      "tsunami: 2\n",
      "watch: 2\n",
      "start: 2\n",
      "los: 2\n",
      "angeles: 2\n",
      "appeal: 2\n",
      "las: 2\n",
      "vegas: 2\n",
      "arena: 2\n",
      "kane: 2\n",
      "end: 2\n",
      "lift: 2\n",
      "stop: 2\n",
      "thrive: 2\n",
      "continue: 2\n",
      "well: 2\n",
      "believe: 2\n",
      "invest: 2\n",
      "read: 1\n",
      "transform: 1\n",
      "wnbarob: 1\n",
      "woollardmon: 1\n",
      "apr: 1\n",
      "min: 1\n",
      "readlink: 1\n",
      "dazzling: 1\n",
      "smash: 1\n",
      "court: 1\n",
      "legacy: 1\n",
      "trailblaze: 1\n",
      "icon: 1\n",
      "secure: 1\n",
      "prepare: 1\n",
      "choose: 1\n",
      "week: 1\n",
      "expert: 1\n",
      "predict: 1\n",
      "transformative: 1\n",
      "cathy: 1\n",
      "commissioner: 1\n",
      "monday: 1\n",
      "generation: 1\n",
      "economic: 1\n",
      "engine: 1\n",
      "ensure: 1\n",
      "financial: 1\n",
      "footing: 1\n",
      "cnbc: 1\n",
      "exist: 1\n",
      "value: 1\n",
      "negotiate: 1\n",
      "hope: 1\n",
      "undervalue: 1\n",
      "enormous: 1\n",
      "landscape: 1\n",
      "change: 1\n",
      "arrival: 1\n",
      "star: 1\n",
      "louisiana: 1\n",
      "state: 1\n",
      "angel: 1\n",
      "kamilla: 1\n",
      "cardoso: 1\n",
      "rivalry: 1\n",
      "johnson: 1\n",
      "larry: 1\n",
      "help: 1\n",
      "create: 1\n",
      "modern: 1\n",
      "think: 1\n",
      "look: 1\n",
      "moment: 1\n",
      "class: 1\n",
      "build: 1\n",
      "significantly: 1\n",
      "substantial: 1\n",
      "social: 1\n",
      "twitter: 1\n",
      "breanna: 1\n",
      "stewart: 1\n",
      "current: 1\n",
      "valuable: 1\n",
      "affectionately: 1\n",
      "nickname: 1\n",
      "bayou: 1\n",
      "barbie: 1\n",
      "business: 1\n",
      "document: 1\n",
      "hawkeyes: 1\n",
      "attendance: 1\n",
      "championship: 1\n",
      "draw: 1\n",
      "average: 1\n",
      "viewer: 1\n",
      "network: 1\n",
      "program: 1\n",
      "generate: 1\n",
      "early: 1\n",
      "sign: 1\n",
      "wash: 1\n",
      "fever: 1\n",
      "club: 1\n",
      "guarantee: 1\n",
      "april: 1\n",
      "sell: 1\n",
      "connecticut: 1\n",
      "hour: 1\n",
      "put: 1\n",
      "sale: 1\n",
      "courtside: 1\n",
      "indianapolis: 1\n",
      "offer: 1\n",
      "resale: 1\n",
      "website: 1\n",
      "cash: 1\n",
      "box: 1\n",
      "office: 1\n",
      "aces: 1\n",
      "announce: 1\n",
      "plan: 1\n",
      "july: 1\n",
      "home: 1\n",
      "michelob: 1\n",
      "ultra: 1\n",
      "big: 1\n",
      "capacity: 1\n",
      "t: 1\n",
      "mobile: 1\n",
      "mary: 1\n",
      "jo: 1\n",
      "professor: 1\n",
      "minnesota: 1\n",
      "found: 1\n",
      "director: 1\n",
      "tucker: 1\n",
      "center: 1\n",
      "research: 1\n",
      "girls: 1\n",
      "unprecedented: 1\n",
      "influence: 1\n",
      "able: 1\n",
      "capture: 1\n",
      "lightning: 1\n",
      "bottle: 1\n",
      "national: 1\n",
      "public: 1\n",
      "radio: 1\n",
      "interview: 1\n",
      "disappointment: 1\n",
      "sunday: 1\n",
      "defeat: 1\n",
      "remain: 1\n",
      "centre: 1\n",
      "attention: 1\n",
      "coach: 1\n",
      "dawn: 1\n",
      "staley: 1\n",
      "thank: 1\n",
      "carry: 1\n",
      "heavy: 1\n",
      "load: 1\n",
      "tour: 1\n",
      "derive: 1\n",
      "satisfaction: 1\n",
      "redefine: 1\n",
      "popular: 1\n",
      "give: 1\n",
      "cool: 1\n",
      "journey: 1\n",
      "achievement: 1\n",
      "include: 1\n",
      "pete: 1\n",
      "maravich: 1\n",
      "scoring: 1\n",
      "play: 1\n",
      "tv: 1\n",
      "investment: 1\n",
      "key: 1\n",
      "success: 1\n",
      "term: 1\n",
      "matter: 1\n",
      "thing: 1\n",
      "money: 1\n",
      "resource: 1\n",
      "drive: 1\n",
      "future: 1\n",
      "sst: 1\n"
     ]
    }
   ],
   "source": [
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process the article text using spaCy\n",
    "doc = nlp(article_text)\n",
    "\n",
    "# Define a set of stopwords\n",
    "stopwords = nlp.Defaults.stop_words\n",
    "\n",
    "# Initialize a Counter to count the frequency of lemmas\n",
    "lemma_counter = Counter()\n",
    "\n",
    "# Iterate over tokens in the document\n",
    "for token in doc:\n",
    "    # Check if the token is a word (not punctuation or whitespace) and not a stop word\n",
    "    if token.is_alpha and not token.is_stop:\n",
    "        # Update the counter with the lowercase version of the token's lemma\n",
    "        lemma_counter[token.lemma_.lower()] += 1\n",
    "\n",
    "# Get the 5 most common lemmas\n",
    "most_common_lemmas = lemma_counter.most_common(5)\n",
    "\n",
    "# Print the 5 most common lemmas with an appropriate label\n",
    "print(\"5 Most Common Lemmas:\")\n",
    "for lemma, frequency in most_common_lemmas:\n",
    "    print(f\"{lemma}: {frequency}\")\n",
    "\n",
    "# Print all lemmas and their frequencies with appropriate labels\n",
    "print(\"\\nAll Lemmas and Their Frequencies:\")\n",
    "for lemma, frequency in lemma_counter.most_common():\n",
    "    print(f\"{lemma}: {frequency}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Make a list containing the scores (using tokens) of every sentence in the article, and plot a histogram with appropriate titles and axis labels of the scores. From your histogram, what seems to be the most common range of scores (put the answer in a comment after your code)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Make a list containing the scores (using lemmas) of every sentence in the article, and plot a histogram with appropriate titles and axis labels of the scores.  From your histogram, what seems to be the most common range of scores (put the answer in a comment after your code)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Using the histograms from questions 5 and 6, decide a \"cutoff\" score for tokens and lemmas such that fewer than half the sentences would have a score greater than the cutoff score.  Record the scores in this Markdown cell\n",
    "\n",
    "* Cutoff Score (tokens): \n",
    "* Cutoff Score (lemmas):\n",
    "\n",
    "Feel free to change these scores as you generate your summaries.  Ideally, we're shooting for at least 6 sentences for our summary, but don't want more than 10 (these numbers are rough estimates; they depend on the length of your article)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Create a summary of the article by going through every sentence in the article and adding it to an (initially) empty list if its score (based on tokens) is greater than the cutoff score you identified in question 8.  If your loop variable is named `sent`, you may find it easier to add `sent.text.strip()` to your list of sentences.  Print the summary (I would cleanly generate the summary text by `join`ing the strings in your list together with a space (`' '.join(sentence_list)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Print the polarity score of your summary you generated with the token scores (with an appropriate label). Additionally, print the number of sentences in the summarized article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Create a summary of the article by going through every sentence in the article and adding it to an (initially) empty list if its score (based on lemmas) is greater than the cutoff score you identified in question 8.  If your loop variable is named `sent`, you may find it easier to add `sent.text.strip()` to your list of sentences.  Print the summary (I would cleanly generate the summary text by `join`ing the strings in your list together with a space (`' '.join(sentence_list)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Print the polarity score of your summary you generated with the lemma scores (with an appropriate label). Additionally, print the number of sentences in the summarized article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12.  Compare your polarity scores of your summaries to the polarity scores of the initial article.  Is there a difference?  Why do you think that may or may not be?.  Answer in this Markdown cell.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Based on your reading of the original article, which summary do you think is better (if there's a difference).  Why do you think this might be?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
